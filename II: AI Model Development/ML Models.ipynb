{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e0b78c3-3f08-47a4-a570-2a274a8e0d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9403c119-1b36-4931-a9b8-f7c60bb2eb2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow pyspark prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2ffa681-5dc7-4859-92b5-e9e9f19cd659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Load Data from Databricks Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf66fb0-d975-4958-b7fb-f0a2fb118559",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_75810f1e\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_8b8ce7b1\",\"enabled\":true,\"columnId\":\"bktxt\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1740147051355}],\"syncTimestamp\":1740147051355}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM test_workspace.kaggle_sap_replicated_data.bkpf_csv LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f5c6b7-e404-4094-ab74-264f687a1afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SAP_CashManagement_AI\").getOrCreate()\n",
    "\n",
    "# Load SAP transactions from Databricks Catalog (assuming BSEG & BKPF are loaded)\n",
    "df_bseg = spark.sql(\"SELECT * FROM test_workspace.kaggle_sap_replicated_data.bseg_csv\")\n",
    "df_bkpf = spark.sql(\"SELECT * FROM test_workspace.kaggle_sap_replicated_data.bkpf_csv\")\n",
    "\n",
    "# Join BSEG & BKPF on Document Number (BELNR), Company Code (BUKRS), and Fiscal Year (GJAHR)\n",
    "df = df_bseg.join(df_bkpf, [\"MANDT\",\"BUKRS\", \"BELNR\", \"GJAHR\"], \"inner\")\n",
    "\n",
    "# Convert posting date to timestamp\n",
    "df = df.withColumn(\"BUDAT\", unix_timestamp(col(\"BUDAT\")).cast(\"timestamp\"))\n",
    "\n",
    "# Display dataset\n",
    "df.select(\"MANDT\",\"BUKRS\", \"GJAHR\", \"BELNR\", \"BUZEI\", \"BUDAT\", \"HKONT\", \"DMBTR\", \"SHKZG\", \"BLART\", \"BSCHL\", \"BKTXT\", \"SGTXT\", \"KOART\", \"KOSTL\", \"PRCTR\", \"LIFNR\", \"KUNNR\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88fa9fbd-7eeb-488e-9d22-5420f0139704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Train Transaction Classification Model\n",
    "Goal: Classify transactions into categories (e.g., Payroll, Vendor Payments, Internal Transfers, Fraud)\n",
    "\n",
    "Algorithm: Logistic Regression (SAP-specific feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f2ee8f9f-170e-4e2f-835a-4d3ecf98c428",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DNU -- old"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Fill NaN values in the input columns\n",
    "df = df.fillna({\n",
    "    \"DMBTR\": 0,\n",
    "    \"HKONT\": 0,\n",
    "    \"KOSTL\": 0,\n",
    "    \"PRCTR\": 0\n",
    "})\n",
    "\n",
    "# Convert categorical fields to indexed numerical values\n",
    "indexer_kostl = StringIndexer(inputCol=\"KOSTL\", outputCol=\"KOSTL_INDEX\", handleInvalid=\"keep\")\n",
    "indexer_prctr = StringIndexer(inputCol=\"PRCTR\", outputCol=\"PRCTR_INDEX\", handleInvalid=\"keep\")\n",
    "indexer_blart = StringIndexer(inputCol=\"BLART\", outputCol=\"BLART_INDEX\", handleInvalid=\"keep\")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"DMBTR\", \"HKONT\", \"KOSTL_INDEX\", \"PRCTR_INDEX\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"  # Handle null values\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"BLART_INDEX\")\n",
    "\n",
    "# Create pipeline (ensure order of transformations)\n",
    "pipeline = Pipeline(stages=[indexer_kostl, indexer_prctr, indexer_blart, assembler, lr])\n",
    "\n",
    "# Ensure the transformations persist\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Verify that the required columns exist before training\n",
    "df_transformed.select(\"BLART_INDEX\", \"KOSTL_INDEX\", \"PRCTR_INDEX\", \"features\").show(5)\n",
    "\n",
    "# Train the model\n",
    "train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "display(predictions.select(\"BUKRS\", \"BELNR\", \"GJAHR\", \"BLART\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6936df26-f9d4-41d1-b74e-9d065c6cc899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    unix_timestamp,\n",
    "    count,\n",
    "    avg,\n",
    "    stddev,\n",
    "    sum,\n",
    "    lag,\n",
    "    datediff,\n",
    "    month,\n",
    "    quarter,\n",
    "    dayofweek,\n",
    "    hour,\n",
    "    countDistinct,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SAP_CashManagement_FraudDetection\").getOrCreate()\n",
    "\n",
    "# Load SAP transactions from Databricks Catalog (assuming BSEG & BKPF are loaded)\n",
    "df_bseg = spark.sql(\"SELECT * FROM test_workspace.kaggle_sap_replicated_data.bseg_csv\")\n",
    "df_bkpf = spark.sql(\"SELECT * FROM test_workspace.kaggle_sap_replicated_data.bkpf_csv\")\n",
    "\n",
    "# Join BSEG & BKPF on Document Number (BELNR), Company Code (BUKRS), and Fiscal Year (GJAHR)\n",
    "df = df_bseg.join(df_bkpf, [\"MANDT\", \"BUKRS\", \"BELNR\", \"GJAHR\"], \"inner\")\n",
    "\n",
    "# Convert posting date to timestamp\n",
    "df = df.withColumn(\"BUDAT\", unix_timestamp(col(\"BUDAT\")).cast(\"timestamp\"))\n",
    "\n",
    "# Fill NULL values in numerical columns with 0\n",
    "df = df.fillna({\"DMBTR\": 0, \"SHKZG\": \"S\"})\n",
    "\n",
    "# Fill NULL values in categorical fields with 'UNKNOWN'\n",
    "df = df.fillna(\n",
    "    {\n",
    "        \"BKTXT\": \"UNKNOWN\",\n",
    "        \"SGTXT\": \"UNKNOWN\",\n",
    "        \"KOART\": \"UNKNOWN\",\n",
    "        \"KOSTL\": \"UNKNOWN\",\n",
    "        \"PRCTR\": \"UNKNOWN\",\n",
    "        \"LIFNR\": \"UNKNOWN\",\n",
    "        \"KUNNR\": \"UNKNOWN\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define window partition by vendor/customer ID\n",
    "window_spec = Window.partitionBy(\"KUNNR\")\n",
    "\n",
    "### FRAUD DETECTION RULES ###\n",
    "\n",
    "# 1️ High-Value Transactions\n",
    "df = df.withColumn(\"High_Amount_Fraud\", when(col(\"DMBTR\") > 50000, 1).otherwise(0))\n",
    "\n",
    "# 2️ Unusual Debit/Credit Ratio\n",
    "df = df.withColumn(\n",
    "    \"Total_Debits\",\n",
    "    sum(when(col(\"SHKZG\") == \"S\", col(\"DMBTR\")).otherwise(0)).over(window_spec),\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Total_Credits\",\n",
    "    sum(when(col(\"SHKZG\") == \"H\", col(\"DMBTR\")).otherwise(0)).over(window_spec),\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Unusual_Debit_Credit_Ratio\",\n",
    "    when(col(\"Total_Debits\") / (col(\"Total_Credits\") + 1) > 5, 1).otherwise(0),\n",
    ")  # Avoid division by zero\n",
    "\n",
    "# 3️ Rapid Sequential Transactions\n",
    "window_spec_ordered = window_spec.orderBy(\"BUDAT\")\n",
    "df = df.withColumn(\n",
    "    \"Time_Diff\",\n",
    "    unix_timestamp(col(\"BUDAT\"))\n",
    "    - unix_timestamp(lag(\"BUDAT\").over(window_spec_ordered)),\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Rapid_Transactions\",\n",
    "    when((col(\"Time_Diff\") < 600) & (col(\"DMBTR\") > 10000), 1).otherwise(0),\n",
    ")\n",
    "\n",
    "# 4️ Duplicate Transactions\n",
    "df_duplicate_txns = (\n",
    "    df.groupBy(\"KUNNR\", \"DMBTR\", \"BUDAT\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"Duplicate_Transactions\")\n",
    ")\n",
    "df = df.join(df_duplicate_txns, [\"KUNNR\", \"DMBTR\", \"BUDAT\"], \"left\")\n",
    "df = df.withColumn(\n",
    "    \"Possible_Duplicate\", when(col(\"Duplicate_Transactions\") > 1, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 5️ Reversal Transactions\n",
    "df = df.withColumn(\n",
    "    \"Potential_Reversal\",\n",
    "    when(lag(\"DMBTR\").over(window_spec_ordered) == -col(\"DMBTR\"), 1).otherwise(0),\n",
    ")\n",
    "\n",
    "# 6️ Transactions Outside Business Hours\n",
    "df = df.withColumn(\"Transaction_Hour\", hour(\"BUDAT\"))\n",
    "df = df.withColumn(\n",
    "    \"After_Hours_Transaction\",\n",
    "    when((col(\"Transaction_Hour\") < 6) | (col(\"Transaction_Hour\") > 20), 1).otherwise(\n",
    "        0\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 7️ High-Risk Vendor/Customer\n",
    "high_risk_vendors = [\"123456\", \"789101\"]  # Example flagged vendors\n",
    "df = df.withColumn(\n",
    "    \"High_Risk_Vendor\", when(col(\"KUNNR\").isin(high_risk_vendors), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 8️ Mismatched Vendor and Account Details\n",
    "df = df.withColumn(\n",
    "    \"Mismatched_Vendor_Account\", when(col(\"LIFNR\") != col(\"HKONT\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 9️ Seasonal Anomaly\n",
    "df = df.withColumn(\"Transaction_Month\", month(\"BUDAT\"))\n",
    "df = df.withColumn(\n",
    "    \"Seasonal_Anomaly\",\n",
    "    when(\n",
    "        (col(\"Transaction_Month\") == 12)\n",
    "        & (col(\"DMBTR\") > 10 * avg(\"DMBTR\").over(window_spec)),\n",
    "        1,\n",
    "    ).otherwise(0),\n",
    ")\n",
    "\n",
    "# 10 Unusual Transaction Count for a Vendor\n",
    "df = df.withColumn(\"Vendor_Transaction_Count\", count(\"DMBTR\").over(window_spec))\n",
    "df = df.withColumn(\n",
    "    \"Unusual_Transaction_Count\",\n",
    "    when(\n",
    "        col(\"Vendor_Transaction_Count\")\n",
    "        > 5 * avg(\"Vendor_Transaction_Count\").over(window_spec),\n",
    "        1,\n",
    "    ).otherwise(0),\n",
    ")\n",
    "\n",
    "### FRAUD SCORE ###\n",
    "fraud_features = [\n",
    "    \"High_Amount_Fraud\",\n",
    "    \"Unusual_Debit_Credit_Ratio\",\n",
    "    \"Rapid_Transactions\",\n",
    "    \"Possible_Duplicate\",\n",
    "    \"Potential_Reversal\",\n",
    "    \"After_Hours_Transaction\",\n",
    "    \"High_Risk_Vendor\",\n",
    "    \"Mismatched_Vendor_Account\",\n",
    "    \"Seasonal_Anomaly\",\n",
    "    \"Unusual_Transaction_Count\",\n",
    "]\n",
    "\n",
    "df = df.withColumn(\"Fraud_Score\", sum([col(f) for f in fraud_features]))\n",
    "\n",
    "# Flag transactions with multiple fraud indicators\n",
    "df = df.withColumn(\n",
    "    \"Fraud_Flag\", when(col(\"Fraud_Score\") >= 3, 1).otherwise(0)\n",
    ")  # Adjust threshold as needed\n",
    "\n",
    "# Encode categorical features\n",
    "indexer_cols = [\"BLART\", \"BSCHL\", \"KOART\", \"KOSTL\", \"PRCTR\", \"LIFNR\", \"KUNNR\"]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\").fit(df)\n",
    "    for col in indexer_cols\n",
    "]\n",
    "\n",
    "for indexer in indexers:\n",
    "    df = indexer.transform(df)\n",
    "\n",
    "# Convert Debit/Credit Indicator (SHKZG) to numerical\n",
    "df = df.withColumn(\"SHKZG_num\", when(col(\"SHKZG\") == \"S\", 1).otherwise(0))\n",
    "\n",
    "# Define features for the model\n",
    "feature_cols = (\n",
    "    fraud_features + [\"Fraud_Score\"] + [col + \"_index\" for col in indexer_cols]\n",
    ")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"keep\"\n",
    ")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Select final columns\n",
    "df = df.select(\"features\", \"Fraud_Flag\")\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Fraud_Flag\", maxIter=10)\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Fraud_Flag\", rawPredictionCol=\"prediction\"\n",
    ")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Model AUC: {auc:.4f}\")\n",
    "\n",
    "# Show some predictions\n",
    "predictions.select(\"features\", \"Fraud_Flag\", \"prediction\", \"probability\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7601ae81-ec28-4c12-aac4-712c9f1a0386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Train Anomaly Detection Model\n",
    "Goal: Detect fraudulent transactions (e.g., duplicate payments, unusual vendor transactions)\n",
    "\n",
    "Algorithm: Isolation Forest (SAP Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188a744f-2c45-4d65-90f6-cd08d21f92df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml import Pipeline\n",
    "import mlflow.spark\n",
    "\n",
    "# Convert string columns to numeric\n",
    "df = df.withColumn(\"KOSTL\", df[\"KOSTL\"].cast(\"double\"))\n",
    "df = df.withColumn(\"PRCTR\", df[\"PRCTR\"].cast(\"double\"))\n",
    "\n",
    "# Scale features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"DMBTR\", \"HKONT\", \"KOSTL\", \"PRCTR\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\"\n",
    ")\n",
    "kmeans = BisectingKMeans(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    k=2\n",
    ")  # k=2 for normal vs anomaly\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "# Train model\n",
    "anomaly_model = pipeline.fit(df)\n",
    "\n",
    "# Predict anomalies\n",
    "anomaly_predictions = anomaly_model.transform(df)\n",
    "display(anomaly_predictions.select(\n",
    "    \"BUKRS\", \"BELNR\", \"GJAHR\", \"DMBTR\", \"KOSTL\", \"PRCTR\", \"prediction\"\n",
    ").limit(10))\n",
    "\n",
    "# Log model\n",
    "mlflow.spark.log_model(anomaly_model, \"sap_anomaly_detection_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "933d0996-6d7f-446f-b6a7-8c9bb411d7ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Train Cash Flow Forecasting Model\n",
    "Goal: Predict future cash inflows and outflows\n",
    "\n",
    "Algorithm: Prophet (Time-Series Model for SAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4039ec81-697d-46cf-95f4-1f64886d6a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "\n",
    "# Convert Spark DataFrame to Pandas (Aggregating cash transactions)\n",
    "cash_flow_data = df.select(\"BLDAT\", \"DMBTR\").groupBy(\"BLDAT\").sum().toPandas()\n",
    "\n",
    "# Rename columns for Prophet\n",
    "cash_flow_data.rename(columns={\"BLDAT\": \"ds\", \"sum(DMBTR)\": \"y\"}, inplace=True)\n",
    "\n",
    "# Train forecasting model\n",
    "model = Prophet()\n",
    "model.fit(cash_flow_data)\n",
    "\n",
    "# Generate future predictions\n",
    "future = model.make_future_dataframe(periods=90)  # Predict for next 90 days\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Display forecast\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "\n",
    "# Save model\n",
    "mlflow.prophet.log_model(model, \"sap_cash_flow_forecasting_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ac0bc96-a633-46fe-b6b9-bf5d0ef7e843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Deploy & Monitor Models\n",
    "Deploy these models for real-time insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b170b1f-5e6b-47a4-8df5-3461b2e3f1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save models for inference\n",
    "model.write().overwrite().save(\"dbfs:/models/sap_transaction_classification\")\n",
    "anomaly_model.write().overwrite().save(\"dbfs:/models/sap_anomaly_detection\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7976065709259951,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ML Models",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
