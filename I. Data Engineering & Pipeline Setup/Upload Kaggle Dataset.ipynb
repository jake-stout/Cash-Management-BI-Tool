{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b028aa3-d387-4ba5-80d1-ca98165645af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Download the Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e007a265-a6b3-4d83-840c-b4fa8db803d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install the Kaggle API"
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58f26e20-195b-4fa6-88a5-94f261f97791",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Move it to DBFS using a Databricks notebook"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Users/jake.stout@stoutdigitalsolutions.com/kaggle.json\", \"dbfs:/tmp/kaggle.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed87613-86fd-42ef-9818-bc44e6af6fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Set Up the Kaggle API in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16803983-19ce-42ae-bf12-1218272c72f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define DBFS path where kaggle.json was uploaded\n",
    "kaggle_json_path = \"/dbfs/tmp/kaggle.json\"\n",
    "\n",
    "# Create the .kaggle directory in the user's home directory\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle/\"), exist_ok=True)\n",
    "\n",
    "# Copy kaggle.json to the correct location\n",
    "shutil.copy(kaggle_json_path, os.path.expanduser(\"~/.kaggle/kaggle.json\"))\n",
    "\n",
    "# Set permissions\n",
    "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec55e1b3-e731-42d4-96f1-476ebb816b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Install and Test the Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c931ea-7f5b-4027-b78d-74622647242f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edc75a39-a815-4444-bf38-1866637965db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Download the SAP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9305b3-699f-45f0-b669-1389281b503d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d mustafakeser4/sap-dataset-bigquery-dataset -p /dbfs/tmp --unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72d8a4a-56cb-47a7-a99d-f809d901297e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Load Dataset into Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da492a7a-0d76-4246-bee5-d6c48ed0a9f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Define source path where Kaggle files are stored\n",
    "source_path = \"dbfs:/tmp/\"\n",
    "\n",
    "# Define target database\n",
    "database_name = \"Kaggle_SAP_Replicated_Data\"\n",
    "\n",
    "# Create the database without specifying a location (Unity Catalog will manage it)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# List all files in the directory\n",
    "files = dbutils.fs.ls(source_path)\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_path = file.path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    table_name = file_name.replace(\".\", \"_\")  # Make table names SQL-friendly\n",
    "    \n",
    "    # Skip non-data files\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Read CSV file\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file_path)\n",
    "    elif file_name.endswith(\".parquet\"):\n",
    "        # Read Parquet file\n",
    "        df = spark.read.parquet(file_path)\n",
    "    else:\n",
    "        print(f\"Skipping {file_name} (unsupported format)\")\n",
    "        continue  # Skip non-data files\n",
    "    \n",
    "    # Write DataFrame to Delta table (Unity Catalog managed location)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.{table_name}\")\n",
    "\n",
    "    print(f\"Loaded {file_name} into {database_name}.{table_name}\")\n",
    "\n",
    "print(\"All files have been processed and stored in the database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1a679d-df7d-4b69-a10d-dc20d5dd887c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM test_workspace.kaggle_sap_replicated_data.bseg_csv l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8178dfa-7e16-4524-9bbb-db28e960a84e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1740027261436}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM test_workspace.kaggle_sap_replicated_data.bseg_csv l\n",
    "JOIN test_workspace.kaggle_sap_replicated_data.bkpf_csv h ON l.mandt=h.mandt AND l.bukrs=h.bukrs AND l.belnr=h.belnr AND l.gjahr=h.gjahr AND h.operation_flag = l.operation_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6750ce13-7d24-428c-b833-356381132d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT b.*\n",
    "FROM test_workspace.kaggle_sap_replicated_data.bseg_csv b\n",
    "LEFT JOIN test_workspace.kaggle_sap_replicated_data.bkpf_csv h \n",
    "ON b.mandt = h.mandt \n",
    "AND b.bukrs = h.bukrs \n",
    "AND b.belnr = h.belnr \n",
    "AND b.gjahr = h.gjahr\n",
    "WHERE h.belnr IS NULL;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8553318355815574,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Upload Kaggle Dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
